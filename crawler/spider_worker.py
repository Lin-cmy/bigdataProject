import redis
import asyncio
import csv
import os
import time
import math
import datetime
import requests
from bilibili_api import video, Credential, sync
# å°è¯•å¯¼å…¥æ ¸å¿ƒè§£ç å™¨
try:
    import dm_pb2
except ImportError:
    print("ç¼ºå°‘ dm_pb2.py æ–‡ä»¶ï¼")
    exit(1)

# ================= é…ç½® =================
REDIS_HOST = 'localhost'
QUEUE_NAME = 'bilibili_tasks'

# ä½ çš„ Cookie (å¿…é¡»å¡«ï¼)
SESSDATA = ""
BILI_JCT = ""
BUVID3 = ""
# =========================================

r = redis.Redis(host=REDIS_HOST, port=6379, db=0, decode_responses=True)

def decode_protobuf(binary_data):
    """
    æ ¸å¿ƒè§£æé€»è¾‘ï¼šå‚è€ƒ danMaKuApi.py
    å°†äºŒè¿›åˆ¶æµè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®åˆ—è¡¨
    """
    try:
        danmaku_seg = dm_pb2.DmSegMobileReply()
        danmaku_seg.ParseFromString(binary_data)
        
        res = []
        for elem in danmaku_seg.elems:
            # å­—æ®µæ˜ å°„å‚è€ƒæºç  src/api/danMaKuApi.py
            res.append({
                'dmid': str(elem.id),
                'video_time': round(elem.progress / 1000.0, 3), # æºç ä¸­ progress å•ä½ä¸ºms
                'text': elem.content,
                'send_date': datetime.datetime.fromtimestamp(elem.ctime).strftime('%Y-%m-%d %H:%M:%S'),
                'uid': elem.midHash
            })
        return res
    except Exception as e:
        print(f"      âš ï¸ Protobuf è§£æå¤±è´¥: {e}")
        return None

def download_and_parse(url, params, save_csv_writer, seen_ids, raw_save_path=None):
    """
    é€šç”¨ä¸‹è½½å‡½æ•°ï¼šæ”¯æŒåŸç”Ÿè¯·æ±‚ + Protobufè§£æ + CSVå†™å…¥ + Binå¤‡ä»½
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Cookie": f"SESSDATA={SESSDATA}"
    }
    
    try:
        # å‘é€è¯·æ±‚
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        
        if resp.status_code == 200 and len(resp.content) > 0:
            # 1. å°è¯•è§£æ
            parsed_data = decode_protobuf(resp.content)
            
            if parsed_data:
                count = 0
                for row in parsed_data:
                    if row['dmid'] not in seen_ids:
                        row['source'] = 'api'
                        save_csv_writer.writerow(row)
                        seen_ids.add(row['dmid'])
                        count += 1
                return True, count
            else:
                # 2. è§£æå¤±è´¥ï¼Œä¿å­˜åŸå§‹ .bin æ–‡ä»¶å…œåº•
                if raw_save_path:
                    with open(raw_save_path, "wb") as f:
                        f.write(resp.content)
                    print(f"      ğŸ’¾ è§£æå¤±è´¥ï¼Œå·²ä¿å­˜åŸå§‹æ•°æ®: {raw_save_path}")
                return False, 0
    except Exception as e:
        print(f"      âŒ è¯·æ±‚å¼‚å¸¸: {e}")
    return False, 0

async def process_video(bvid):
    print(f"ğŸ¤– [Worker] å¤„ç†ä»»åŠ¡: {bvid} ...")
    
    output_csv = f"danmaku_{bvid}.csv"
    raw_dir = f"raw_data_{bvid}"
    if not os.path.exists(raw_dir): os.makedirs(raw_dir)

    cred = Credential(sessdata=SESSDATA, bili_jct=BILI_JCT, buvid3=BUVID3)
    v = video.Video(bvid=bvid, credential=cred)

    # åˆå§‹åŒ– CSV
    headers = ['dmid', 'video_time', 'text', 'send_date', 'uid', 'source']
    seen_dmids = set()
    
    # æ–­ç‚¹ç»­ä¼ ï¼šè¯»å–å·²æœ‰ ID
    if os.path.exists(output_csv):
        try:
            with open(output_csv, 'r', encoding='utf-8-sig') as f:
                for row in csv.DictReader(f): seen_dmids.add(row['dmid'])
        except: pass

    f = open(output_csv, 'a+', encoding='utf-8-sig', newline='')
    writer = csv.DictWriter(f, fieldnames=headers)
    if os.path.getsize(output_csv) == 0: writer.writeheader()

    try:
        info = await v.get_info()
        title = info['title']
        duration = info['duration']
        cid = info['cid']
        pub_date = datetime.date.fromtimestamp(info['ctime'])
        
        # ç®—æ³•ï¼šå‚è€ƒæºç  utils/yearDaysUtils.py è®¡ç®—åˆ†åŒ…é€»è¾‘
        total_segments = math.ceil(duration / 360.0)
        print(f"   ğŸ“º è§†é¢‘: {title} (CID={cid}) | åˆ†åŒ…æ•°: {total_segments}")

        total_new = 0

        # === é˜¶æ®µä¸€ï¼šåˆ†åŒ…æ‰«æ (API: web/seg.so) ===
        print(f"   ğŸš€ [é˜¶æ®µä¸€] åˆ†åŒ…æ‰«æ...")
        for i in range(1, total_segments + 1):
            url = "https://api.bilibili.com/x/v2/dm/web/seg.so"
            params = {"type": 1, "oid": cid, "segment_index": i}
            
            # ä¸‹è½½å¹¶è§£æ
            success, count = download_and_parse(url, params, writer, seen_dmids, 
                                              raw_save_path=os.path.join(raw_dir, f"seg_{i}.bin"))
            if success:
                print(f"      âœ… åˆ†åŒ… {i}: +{count} æ¡")
                total_new += count
            else:
                print(f"      âš ï¸ åˆ†åŒ… {i} å¤±è´¥æˆ–æ— æ•°æ®")
            
            time.sleep(0.8)

        # === é˜¶æ®µäºŒï¼šå†å²å›æº¯ (API: web/history/seg.so) ===
        print(f"   ğŸš€ [é˜¶æ®µäºŒ] å†å²å›æº¯...")
        # 1. è·å–æœ‰å¼¹å¹•çš„æ—¥æœŸ
        today = datetime.date.today()
        target_months = []
        for k in range(12): # æŸ¥æœ€è¿‘1å¹´
            d = today - datetime.timedelta(days=30*k)
            if d < pub_date and d.month != pub_date.month: break
            target_months.append(d.strftime("%Y-%m"))
        
        # è·å–ç´¢å¼•
        valid_dates = []
        for m in sorted(list(set(target_months))):
            try:
                # ä½¿ç”¨ requests ç›´æ¥æŸ¥ç´¢å¼•ï¼Œé¿å¼€åº“ç‰ˆæœ¬é—®é¢˜
                idx_url = "https://api.bilibili.com/x/v2/dm/history/index"
                idx_resp = requests.get(idx_url, params={"type":1, "oid":cid, "month":m}, 
                                      headers={"Cookie": f"SESSDATA={SESSDATA}"})
                if idx_resp.json()['code'] == 0 and idx_resp.json()['data']:
                    valid_dates.extend(idx_resp.json()['data'])
            except: pass
            time.sleep(0.5)

        print(f"      ğŸ“… å‘ç° {len(valid_dates)} ä¸ªå†å²æ—¥æœŸ")
        
        # 2. ä¸‹è½½å†å²å¼¹å¹•
        for date_str in valid_dates:
            url = "https://api.bilibili.com/x/v2/dm/web/history/seg.so"
            params = {"type": 1, "oid": cid, "date": date_str}
            
            success, count = download_and_parse(url, params, writer, seen_dmids,
                                              raw_save_path=os.path.join(raw_dir, f"hist_{date_str}.bin"))
            if count > 0:
                print(f"      â¬‡ï¸ {date_str}: +{count} æ¡")
                total_new += count
            time.sleep(1.2) # å†å²æ¥å£é™é€Ÿ

        print(f"âœ… [Worker] {bvid} ç»“æŸã€‚æ€»å…¥åº“: {total_new} æ¡")

    except Exception as e:
        print(f"âŒ ä»»åŠ¡å‡ºé”™: {e}")

    f.close()

async def main_loop():
    print("ğŸš€ åˆ†å¸ƒå¼çˆ¬è™« Worker Pro å¯åŠ¨...")
    while True:
        task = r.brpop(QUEUE_NAME, timeout=30)
        if task: await process_video(task[1])

if __name__ == '__main__':
    try: loop = asyncio.get_event_loop()
    except: loop = asyncio.new_event_loop(); asyncio.set_event_loop(loop)
    sync(main_loop())