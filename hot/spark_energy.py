import sys
import os
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, floor, count, sum as spark_sum, udf, percent_rank, input_file_name, regexp_extract, collect_list, struct
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.window import Window

# ================= é…ç½®åŒºåŸŸ =================
# è¾“å…¥è·¯å¾„ï¼šåŒ¹é…æ‰€æœ‰åˆ†é›†çš„ CSV æ–‡ä»¶
INPUT_PATH = "danmaku_*.csv"  # ç¡®ä¿è¿™é‡Œèƒ½åŒ¹é…åˆ°æ‰€æœ‰37é›†çš„æ–‡ä»¶
# è¾“å‡ºæ–‡ä»¶
OUTPUT_JSON = "all_episodes_energy.json"
# æ—¶é—´çª—å£ (10ç§’)
WINDOW_SIZE = 10 
# ===========================================

def main():
    spark = SparkSession.builder \
        .appName("SpyFamilyBatchEnergy") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()
    
    # 1. è¯»å–æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶ä¿ç•™æ–‡ä»¶åä½œä¸º"é›†æ•°ID"
    # input_file_name() ä¼šè·å–å®Œæ•´çš„ hdfs://.../danmaku_BVxxx.csv è·¯å¾„
    raw_df = spark.read.csv(INPUT_PATH, header=True, inferSchema=True) \
        .withColumn("filepath", input_file_name())
    
    # ä»æ–‡ä»¶åä¸­æå– BVID (å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º danmaku_BVxxxx.csv)
    # æ­£åˆ™æå– BV å·
    df = raw_df.withColumn("bvid", regexp_extract(col("filepath"), r"(BV[a-zA-Z0-9]+)", 1)) \
               .filter(col("video_time").cast("double").isNotNull())

    # 2. ä¸“å±é«˜èƒ½æƒ…æ„Ÿè¯å…¸ (åŸºäºä½ çš„ CSV ç»Ÿè®¡æ•°æ®å®šåˆ¶)
    def get_weighted_score(text):
        if not text: return 1.0
        t = text.lower()
        
        # [Tier 1] æ ¸å¿ƒæ¢— (æƒé‡ x5.0) - åŸºäº CSV é¢‘ç‡Topè¯
        god_tier = ['ä¼˜é›…', 'elegance', 'å“‡åº“', 'å“‡é…·', 'waku', 'ç“œç¥', 'ä¸–ç•Œåç”»', 'ååœºé¢']
        # [Tier 2] è§’è‰²ä¸å‰§æƒ… (æƒé‡ x3.0)
        high_tier = ['æ¬¡å­', 'æ˜çˆ¹', 'çˆ¶äº²', 'çº¦å°”', 'å¤ªå¤ª', 'è†æ£˜å…¬ä¸»', 'é‚¦å¾·', 'ä¸Šå²¸', 'è¯»å¿ƒ', 'èŠ±ç”Ÿ', 'åƒèŠ±ç”Ÿ', 'æ‰‹é›·', 'æ¯äº²', 'å¦ˆå¦ˆ', 'é˜¿å°¼äºš']
        # [Tier 3] é€šç”¨æƒ…ç»ª (æƒé‡ x2.0)
        mid_tier = ['é«˜èƒ½', 'æ³ªç›®', 'èµ·ç«‹', 'å§æ§½', 'ç‰›é€¼', 'awsl', 'å°ç¥', 'è‡´æ•¬', 'å®Œç»“', 'æ’’èŠ±', 'å¥½å¸…', 'å¯çˆ±']
        # [Tier 4] å™ªéŸ³é™æƒ (æƒé‡ x0.5)
        noise_tier = ['å“ˆå“ˆ', 'hhh', 'www', '233', 'æ‰“å¡', 'ç¬¬ä¸€', 'çƒ­ä¹']

        for w in god_tier:
            if w in t: return 5.0
        for w in high_tier:
            if w in t: return 3.0
        for w in mid_tier:
            if w in t: return 2.0
        for w in noise_tier:
            if w in t: return 0.5
            
        return 1.0

    score_udf = udf(get_weighted_score, DoubleType())

    # 3. è®¡ç®—åŸºç¡€å¾—åˆ†
    # å¢åŠ  "bvid" åˆ†ç»„ç»´åº¦
    window_df = df.withColumn("raw_score", score_udf(col("text"))) \
                  .withColumn("time_bucket", (floor(col("video_time") / WINDOW_SIZE) * WINDOW_SIZE).cast("int")) \
                  .groupBy("bvid", "time_bucket") \
                  .agg(
                      count("dmid").alias("density"),
                      spark_sum("raw_score").alias("sentiment_score")
                  )

    # 4. ç»¼åˆè¯„åˆ†
    final_df = window_df.withColumn("energy", col("sentiment_score") * 0.37 + col("density") * 0.63)

    # 5. ã€å…³é”®ã€‘æŒ‰é›†æ•°åˆ†ç»„è®¡ç®—æ’å
    # æˆ‘ä»¬éœ€è¦åœ¨"æ¯ä¸€é›†å†…éƒ¨"æ‰¾å‡º Top 10% çš„æ—¶åˆ»ï¼Œè€Œä¸æ˜¯è·Ÿåˆ«çš„é›†æ¯”
    w = Window.partitionBy("bvid").orderBy("energy")
    
    ranked_df = final_df.withColumn("rank_pct", percent_rank().over(w)) \
                        .withColumn("is_highlight", col("rank_pct") >= 0.9) # Top 10%

    # 6. ç»“æ„åŒ–è¾“å‡º
    # æˆ‘ä»¬éœ€è¦æŠŠæ•°æ®è½¬æ¢æˆå‰ç«¯å¥½ç”¨çš„æ ¼å¼ï¼š
    # [ { "bvid": "BV1xx...", "timeline": [ { "time": 0, "value": 10 }, ... ] }, ... ]
    
    # å…ˆæŒ‰æ—¶é—´æ’åº
    sorted_df = ranked_df.orderBy("bvid", "time_bucket")
    
    # èšåˆæ¯ä¸€é›†çš„æ•°æ®
    output_structure = sorted_df.groupBy("bvid") \
        .agg(collect_list(struct(
            col("time_bucket").alias("time"),
            col("energy").alias("value"),
            col("is_highlight").alias("high")
        )).alias("timeline"))

    # 7. å¯¼å‡º JSON
    print("ğŸš€ æ­£åœ¨èšåˆ 37 é›†æ•°æ®å¹¶å¯¼å‡º...")
    results = output_structure.collect()
    
    final_json = []
    for row in results:
        final_json.append({
            "bvid": row["bvid"],
            # ç®€å•çš„é›†æ•°æ˜ å°„é€»è¾‘ï¼Œå®é™…å¯ä»¥ç”¨å­—å…¸æ˜ å°„ BVID -> ç¬¬å‡ é›†
            "title": f"è§†é¢‘ {row['bvid']}", 
            "timeline": [
                {
                    "time": item.time,
                    "value": round(item.value, 2),
                    "is_high": item.high
                } for item in row.timeline
            ]
        })

    with open(OUTPUT_JSON, "w", encoding='utf-8') as f:
        json.dump(final_json, f, ensure_ascii=False)

    print(f"âœ… æ‰¹å¤„ç†å®Œæˆï¼å·²ç”Ÿæˆ {len(final_json)} é›†çš„é«˜èƒ½æ•°æ®ã€‚")
    spark.stop()

if __name__ == "__main__":
    main()