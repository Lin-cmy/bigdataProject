# -*- coding: utf-8 -*-
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, udf, count, desc
from pyspark.sql.types import ArrayType, StringType

# ================= é…ç½®åŒºåŸŸ =================
# 1. è¾“å…¥æ•°æ®è·¯å¾„ (HDFSè·¯å¾„)
# è¯·ç¡®ä¿ä½ å·²ç»æŠŠçˆ¬ä¸‹æ¥çš„csvä¸Šä¼ åˆ°äº†è¿™é‡Œï¼Œæ”¯æŒé€šé…ç¬¦ *.csv
INPUT_PATH = "/root/home/p1/family/*.csv" 

# 2. è¾“å‡ºè·¯å¾„ (HDFSè·¯å¾„)
# ç»“æœå°†ä¿å­˜åˆ°è¿™é‡Œ
OUTPUT_PATH = "/root/home/p1/output/high_energy_words"
# ===========================================

def main():
    # åˆå§‹åŒ– Spark
    # åˆå§‹åŒ– Spark
    spark = SparkSession.builder \
        .appName("DanmakuPreprocessing") \
        .getOrCreate()
    
    # æ‰“å°æ—¥å¿—
    print(f"ğŸš€ [é¢„å¤„ç†] å¼€å§‹è¯»å–æ•°æ®: {INPUT_PATH}")

    try:
        # è¯»å– CSV æ–‡ä»¶
        # header=True è¡¨ç¤ºç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
        # inferSchema=True è¡¨ç¤ºè‡ªåŠ¨æ¨æ–­å­—æ®µç±»å‹
        df = spark.read.csv(INPUT_PATH, header=True, inferSchema=True)
        
        # æ‰“å°ä¸€ä¸‹æ•°æ®ç»“æ„ï¼Œç¡®è®¤æ²¡è¯»é”™
        df.printSchema()
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥HDFSè·¯å¾„ã€‚é”™è¯¯: {e}")
        return

    # å®šä¹‰åˆ†è¯å‡½æ•° (è¿è¡Œåœ¨æ¯ä¸€ä¸ª Executor ä¸Š)
    def seg_text(text):
        import jieba
        if not text: 
            return []
        
        # è‡ªå®šä¹‰åœç”¨è¯ (è¿‡æ»¤æ‰æ²¡ç”¨çš„æ°´è¯)
        stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
            'å•Š', 'å§', 'å‘€', 'å˜›', 'å‘¢', 'å“ˆ', 'å“¦', 'å—¯'
        }
        
        # åˆ†è¯
        words = jieba.cut(text)
        
        # è¿‡æ»¤é€»è¾‘ï¼š
        # 1. ä¸åœ¨åœç”¨è¯è¡¨ä¸­
        # 2. é•¿åº¦å¤§äº1 (å•ä¸ªå­—é€šå¸¸å¾ˆéš¾ä»£è¡¨é«˜èƒ½æƒ…ç»ªï¼Œé™¤éæ˜¯"è‰")
        # 3. ä¸æ˜¯çº¯æ•°å­—
        return [w for w in words if w not in stop_words and len(w) > 1 and not w.isnumeric()]

    # æ³¨å†Œ UDF (User Defined Function)
    seg_udf = udf(seg_text, ArrayType(StringType()))

    print("ğŸ” [é¢„å¤„ç†] æ­£åœ¨è¿›è¡Œåˆ†è¯ä¸æ¸…æ´—...")
    
    # æ ¸å¿ƒå¤„ç†æµç¨‹
    # 1. è¿‡æ»¤ç©ºå¼¹å¹•
    # 2. å¯¹ 'text' åˆ—è¿›è¡Œåˆ†è¯ï¼Œç”Ÿæˆæ–°åˆ— 'words'
    # 3. explode å°†ä¸€è¡Œåˆ—è¡¨ [A, B] ç‚¸å¼€æˆä¸¤è¡Œ A, B (æ–¹ä¾¿ç»Ÿè®¡)
    words_df = df.filter(col("text").isNotNull()) \
                 .withColumn("words", seg_udf(col("text"))) \
                 .select(explode(col("words")).alias("word"))
    
    # ç»Ÿè®¡è¯é¢‘
    print("ğŸ“Š [é¢„å¤„ç†] æ­£åœ¨ç»Ÿè®¡è¯é¢‘...")
    word_counts = words_df.groupBy("word") \
                          .agg(count("word").alias("frequency")) \
                          .orderBy(desc("frequency")) \
                          .limit(500) # åªå–å‰500ä¸ªé«˜é¢‘è¯ï¼Œå¤ªå¤šäº†æ²¡æ„ä¹‰

    # å±•ç¤ºå‰ 20 ä¸ªç»“æœåˆ°æ§åˆ¶å°
    print("ğŸ† Top 20 é«˜èƒ½å€™é€‰è¯:")
    word_counts.show(20, truncate=False)
    
    # ä¿å­˜ç»“æœåˆ° HDFS
    # ä½¿ç”¨ coalesce(1) å°†ç»“æœåˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥çœ‹
    word_counts.coalesce(1).write.mode("overwrite").csv(OUTPUT_PATH, header=True)
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ HDFS: {OUTPUT_PATH}")
    spark.stop()

if __name__ == "__main__":
    main()